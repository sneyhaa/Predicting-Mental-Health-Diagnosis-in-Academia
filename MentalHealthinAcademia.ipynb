{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7peCZ1eMFxiw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict whether they will be diagnosed with mental health disorder\n",
        "df = pd.read_csv('./02_Student_Mental_Health_2021-10-10.csv')\n",
        "data = df.loc[df[\"Diagnosis\"] < 3.0 ]\n",
        "\n",
        "miss_data = data.isnull().sum() * 100 / len(data)\n",
        "drops = []\n",
        "for count, val in enumerate(miss_data):\n",
        "    if val != 0:\n",
        "        drops.append(count)\n",
        "\n",
        "drops = [0, 1, 2, 3, 6, 21, 118]\n",
        "data = data.drop(data.columns[drops], axis=1)\n",
        "\n",
        "col = data.pop(\"Diagnosis\")\n",
        "# insert column with insert(location, column_name, column_value)\n",
        "data = data.assign(Diagnosis=col)\n",
        "\n",
        "# Make train and test sets\n",
        "x = data.iloc[:,:139]\n",
        "y = data.iloc[:, 139]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=500)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tree.DecisionTreeClassifier(random_state=500)\n",
        "model = model.fit(x_train, y_train)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "feature_importances = pd.DataFrame({'Feature': x_train.columns, 'Importance': importances})\n",
        "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
        "\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "id": "0Z4S6GtHF6ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyp_params_RF = {'n_estimators': [20, 50, 100, 300, 500, 1000, 2000], 'random_state':[25, 125, 200, 300, 500]}\n",
        "# Make train and test sets\n",
        "x = data.iloc[:,:139]\n",
        "y = data.iloc[:, 139]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=500)\n",
        "\n",
        "model_RF = RandomForestClassifier(n_estimators=2250, oob_score=True, random_state=300)\n",
        "model_RF = model.fit(x_train, y_train)\n",
        "pred = model_RF.predict(x_test)\n",
        "print(roc_auc_score(y_test, pred))\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "id": "ESG6ZIbbF9ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.iloc[:,:139]\n",
        "y = data.iloc[:, 139]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=500)\n",
        "\n",
        "model_RF = RandomForestClassifier(n_estimators=2250, oob_score=True, random_state=300)\n",
        "model_RF = model.fit(x_train, y_train)\n",
        "\n",
        "importances = model_RF.feature_importances_\n",
        "feature_importances = pd.DataFrame({'Feature': x_train.columns, 'Importance': importances})\n",
        "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
        "\n",
        "#print(feature_importances['Importance'])\n",
        "\n",
        "chosen_features_big = feature_importances.loc[feature_importances['Importance'] > 0.002]\n",
        "chosen_features_med = feature_importances.loc[feature_importances['Importance'] > 0.005]\n",
        "chosen_features_sml = feature_importances.loc[feature_importances['Importance'] > 0.011]\n",
        "\n",
        "feat_sml = []\n",
        "feat_med = []\n",
        "feat_big = []\n",
        "\n",
        "for i in chosen_features_sml['Feature']:\n",
        "    feat_sml.append(i)\n",
        "feat_sml.append('Diagnosis')\n",
        "\n",
        "for i in chosen_features_med['Feature']:\n",
        "    feat_med.append(i)\n",
        "feat_med.append('Diagnosis')\n",
        "\n",
        "for i in chosen_features_big['Feature']:\n",
        "    feat_big.append(i)\n",
        "feat_big.append('Diagnosis')\n",
        "\n",
        "# Make new dataframe with only the important features\n",
        "df_best_sml = data[feat_sml]\n",
        "df_best_med = data[feat_med]\n",
        "df_best_big = data[feat_big]\n",
        "df_best_med.shape\n"
      ],
      "metadata": {
        "id": "pYSN4luaF-cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make new train and test set\n",
        "# Make train and test sets\n",
        "x = df_best_sml.iloc[:,:22]\n",
        "y = df_best_sml.iloc[:, 22]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=500)\n",
        "\n",
        "model_RF = RandomForestClassifier(n_estimators=2250, oob_score=True, random_state=300)\n",
        "model_RF = model_RF.fit(x_train, y_train)\n",
        "pred = model_RF.predict(x_test)\n",
        "#print(roc_auc_score(y_test, pred))\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "id": "UOZLTTT1GFSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make new train and test set\n",
        "# Make train and test sets\n",
        "x = df_best_med.iloc[:,:64]\n",
        "y = df_best_med.iloc[:, 64]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=500)\n",
        "\n",
        "model_RF = RandomForestClassifier(n_estimators=2250, oob_score=True, random_state=300)\n",
        "model_RF = model_RF.fit(x_train, y_train)\n",
        "pred = model_RF.predict(x_test)\n",
        "print(roc_auc_score(y_test, pred))\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "id": "iAck3Ok5GJGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make new train and test set\n",
        "# Make train and test sets\n",
        "x = df_best_big.iloc[:,:52]\n",
        "y = df_best_big.iloc[:, 52]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=500)\n",
        "\n",
        "model_RF = RandomForestClassifier(n_estimators=1000, oob_score=True, random_state=300)\n",
        "model_RF = model_RF.fit(x_train, y_train)\n",
        "pred = model_RF.predict(x_test)\n",
        "print(roc_auc_score(y_test, pred))\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "id": "331ABwrzGJnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(range(len(importances)), importances)\n",
        "#plt.xticks(range(len(importances)), y, rotation=90)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t1bCD0IIGLoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform grid search\n",
        "hyp_params_GB = {'n_estimators': [20, 50, 100, 300, 500, 1000, 2000], 'learning_rate':[0.9, 0.5, 0.3, 0.1, 0.01, 0.001],\n",
        "             'random_state':[25, 125, 200, 300, 500]}\n",
        "model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.05, random_state=300)\n",
        "model = model.fit(x_train, y_train)\n",
        "importances = model_RF.feature_importances_\n",
        "feature_importances = pd.DataFrame({'Feature': x_train.columns, 'Importance': importances})\n",
        "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
        "chosen_features_big = feature_importances.loc[feature_importances['Importance'] > 0.002]\n",
        "chosen_features_med = feature_importances.loc[feature_importances['Importance'] > 0.005]\n",
        "chosen_features_sml = feature_importances.loc[feature_importances['Importance'] > 0.01]\n",
        "#pred = model.predict(x_test)\n",
        "#print(roc_auc_score(y_test, pred))\n",
        "#print(accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "id": "qoOjaam4GPIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(range(len(importances)), importances)\n",
        "#plt.xticks(range(len(importances)), y, rotation=90)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KYJYqun1GRKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "model = MLPClassifier(activation='logistic',alpha=.01, hidden_layer_sizes=(3, 5),max_iter=5000)\n",
        "model = model.fit(x_train, y_train)\n",
        "pred = model.predict(x_test)\n",
        "print(roc_auc_score(y_test, pred))\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "id": "AFvXBqfjGTNQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}